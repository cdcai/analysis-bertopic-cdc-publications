{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02832a98",
   "metadata": {},
   "source": [
    "# Reproducibility\n",
    "\n",
    "The model used in our paper is not entirely reproducible. In this notebook, we will outline reasons this is the case and why your mileage may vary when attempting to reproduce our results using the Jupyter notebook (2024_CDC_Topic_Model_Code_Workbook.ipynb).\n",
    "\n",
    "## 1. Library Dependencies:\n",
    "Results will vary depending on your versions of Python and other libraries used to generate the model. If you want to match libraries and dependencies as close as possible, please consult Topic_Modeling_Library_Versions.txt.\n",
    "\n",
    "## 2. UMAP is inherently stochastic:\n",
    "Within the same environment, setting the random state of UMAP should ensure the same results are generated every time the notebook is run. However, when switching between different environments (different OS, etc.), this will not be the case. With the random state set the same as in our Jupyter notebook (2024_CDC_Topic_Model_Code_Workbook.ipynb), you will not reproduce our UMAP results, which means the final topic model results will also be different.\n",
    "\n",
    "For more discussion about this issue, please see:\n",
    "- https://github.com/MaartenGr/BERTopic/issues/559\n",
    "- https://github.com/lmcinnes/umap/issues/153\n",
    "\n",
    "## 3. Our loaded model is a reduced model and will categorize the same documents differently:\n",
    "We saved our model to a light save format (see more info: https://maartengr.github.io/BERTopic/getting_started/serialization/serialization.html). This means this model did not save the full version of UMAP and HDBSCAN.\n",
    "\n",
    "Instead, it is using cosine similarity between the cluster representations (from keywords) and the document embeddings (see documentation of the transform function of BERTopic here: https://github.com/MaartenGr/BERTopic/blob/master/bertopic/_bertopic.py). This reduced model usually results in fewer publications being classified as unclustered.\n",
    "\n",
    "Please consult example code below for loading the reduced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0180a2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Libraries and Others\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca43738",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c11492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from transformers.pipelines import pipeline\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc99b2e",
   "metadata": {},
   "source": [
    "Here, we import Science Clips. \n",
    "\n",
    "Science Clips is available as an Excel download from: https://www.cdc.gov/library/sciclips/download/.\n",
    "\n",
    "Because we used a specific version of Science Clips (accessed 5/3/2024), your results may vary if you use a fresh copy of Science Clips; to best replicate our results, limit the \"Date\" field of your download to 5/3/2024 or earlier. \n",
    "\n",
    "Our copy has a large file size that isn't stored in this GitHub repo. If you would like a copy of the specific version of Science Clips we used, please contact us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1c1842",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clips = pd.read_excel('ScienceClips_accessed20240503.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97cf924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will limit my dataset to publications from 2014 to 2023\n",
    "df_clips = df_clips[df_clips['Year'] <= 2023]\n",
    "df_clips = df_clips[df_clips['Year'] >= 2014]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2274996a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clips.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3335f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(df_clips['Title'] + ' ' + df_clips['Abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e396f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f89684",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [str(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f18bb45",
   "metadata": {},
   "source": [
    "### At this step, we embed the documents using the same model as the full model originally used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f82944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"sentence-transfomers/allenai-specter\")\n",
    "embeddings = embedding_model.encode(docs, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89eba31",
   "metadata": {},
   "source": [
    "### At this step, we load the reduced model from the directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46299b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load from directory\n",
    "loaded_model = BERTopic.load(\"FinalModel_SPECTER_20240824\",embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c7ff7",
   "metadata": {},
   "source": [
    "### In the loaded model, we can see the original counts in each cluster from the full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0418b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36b9e07",
   "metadata": {},
   "source": [
    "### We can also see the original topic labels for each document (in the same order as initial input):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da131f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In BERTopic v0.9.2 or higher:\n",
    "topics_original = loaded_model.topics_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e93d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63a6692",
   "metadata": {},
   "source": [
    "### However, if we transform the documents using the reduced model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32720e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics,probs = loaded_model.transform(docs,embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb05eb4",
   "metadata": {},
   "source": [
    "### We see that fewer documents are unclustered (-1) and counts for other clusters have changed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ded2f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(topics).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f77a98",
   "metadata": {},
   "source": [
    "The reduced model moves many publications from being unclustered into various topics. It works reasonably well, though doesn't behave the same as the full model. We checked a random sample of 100 publications that changed labels in three categories: from unclustered to clustered, from clustered to unclustered, and from one cluster to another. Here's a summary of what we found:\n",
    "\n",
    "- If it moved an unclustered (\"noise\") paper to a cluster, it seemed reasonable about 82% of the time in the random sample. That's not bad given how much noisy-ness there was in the clusters to start with.\n",
    "- If it moved a clustered paper to unclustered (\"noise\"), it seemed like a reasonable choice 34% of the time (about 1 in 3) in the random sample. Most of the time the paper looked like it did belong in the cluster. However, this happened less than 300 times in the dataset (0.1% of papers), so the behavior was rare.\n",
    "- If it moved a clustered paper to another cluster, it seemed reasonable about 72% of the time, which I think is good. There were some super obvious case examples, like a paper titled \"Diabetes and tuberculosis in the Pacific Islands region\" moving from \"Tuberculosis\" to \"Diabetes or Cardiovascular Health\"; where it's clear the paper could be one or both."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
